{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section II: Pre-processign & hybrid deep learning structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. take out pre-trial signals from all C channels and cut it into N segmentations with length L --> N (C*L matrixes)\n",
    "2. element-wise addition (matrixes) & mean value --> BaseMean (C*L matrixes) --> basic emotion state\n",
    "3. RawEEG: segment raw EEG into M (C*L matrixes)\n",
    "4. BaseRemoved = RawEEG - BaseMean\n",
    "5. concatenate all BaseRermoved matrixes into a big matrix (size the same with raw EEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEAP dataset: n=32; h=9 (vertical); w=9 (horizontal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Converting 1D EEG into 2D EEG frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def data_1Dto2D(data, Y=9, X=9):\n",
    "    data_2D = np.zeros([Y, X])\n",
    "    data_2D[0] = (0,        0,          0,          data[0],    0,          data[16],   0,          0,          0       )\n",
    "    data_2D[1] = (0,        0,          0,          data[1],    0,          data[17],   0,          0,          0       )\n",
    "    data_2D[2] = (data[3],  0,          data[2],    0,          data[18],   0,          data[19],   0,          data[20])\n",
    "    data_2D[3] = (0,        data[4],    0,          data[5],    0,          data[22],   0,          data[21],   0       )\n",
    "    data_2D[4] = (data[7],  0,          data[6],    0,          data[23],   0,          data[24],   0,          data[25])\n",
    "    data_2D[5] = (0,        data[8],    0,          data[9],    0,          data[27],   0,          data[26],   0       )\n",
    "    data_2D[6] = (data[11], 0,          data[10],   0,          data[15],   0,          data[28],   0,          data[29])\n",
    "    data_2D[7] = (0,        0,          0,          data[12],   0,          data[30],   0,          0,          0       )\n",
    "    data_2D[8] = (0,        0,          0,          data[13],   data[14],   data[31],   0,          0,          0       )\n",
    "    # return shape:9*9\n",
    "    return data_2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalization\n",
    "# Z = (x - u) / sigma\n",
    "# x: a non-zero element from a certain position of the frame\n",
    "# u: the mean of all non-zero elements\n",
    "# sigma: the stand deviation of these elements\n",
    "\n",
    "def feature_normalize(data):\n",
    "    mean = data[data.nonzero()].mean()\n",
    "    sigma = data[data. nonzero ()].std()\n",
    "    data_normalized = data\n",
    "    data_normalized[data_normalized.nonzero()] = (data_normalized[data_normalized.nonzero()] - mean)/sigma\n",
    "    # return shape: 9*9\n",
    "    return data_normalized\n",
    "\n",
    "def norm_dataset(dataset_1D):\n",
    "    norm_dataset_1D = np.zeros([dataset_1D.shape[0], 32])\n",
    "    for i in range(dataset_1D.shape[0]):\n",
    "        norm_dataset_1D[i] = feature_normalize(dataset_1D[i])\n",
    "    # return shape: m*32\n",
    "    return norm_dataset_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_1Dto2D(dataset_1D):\n",
    "    dataset_2D = np.zeros([dataset_1D.shape[0],9,9])\n",
    "    for i in range(dataset_1D.shape[0]):\n",
    "        dataset_2D[i] = data_1Dto2D(dataset_1D[i])\n",
    "    # return shape: m*9*9\n",
    "    return dataset_2D\n",
    "\n",
    "def norm_dataset_1Dto2D(dataset_1D):\n",
    "    norm_dataset_2D = np.zeros([dataset_1D.shape[0], 9, 9])\n",
    "    for i in range(dataset_1D.shape[0]):\n",
    "        norm_dataset_2D[i] = feature_normalize( data_1Dto2D(dataset_1D[i]))\n",
    "    # return shape: m*9*9\n",
    "    return norm_dataset_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while ((start+size) < data.shape[0]):\n",
    "        yield int(start), int(start + size)\n",
    "        start += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_signal_without_transition(data,label,label_index,window_size):\n",
    "    # get data file name and label file name\n",
    "    for (start, end) in windows(data, window_size):\n",
    "        # print(data.shape)\n",
    "        if((len(data[start:end]) == window_size)):\n",
    "            if(start == 0):\n",
    "                segments = data[start:end]\n",
    "                segments = np.vstack([segments, data[start:end]])\n",
    "\n",
    "                labels = np.array(label[label_index])\n",
    "                labels = np.append(labels, np.array(label[label_index]))\n",
    "            else:\n",
    "                segments = np.vstack([segments, data[start:end]])\n",
    "                labels = np.append(labels, np.array(label[label_index])) # labels = np.append(labels, stats.mode(label[start:end])[0][0])\n",
    "    return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mixup(dataset_file,window_size,label,yes_or_not): # initial empty label arrays\n",
    "    print(\"Processing\",dataset_file,\"..........\")\n",
    "    data_file_in = sio.loadmat(dataset_file)\n",
    "    data_in = data_file_in[\"data\"].transpose(0,2,1)\n",
    "    #0 valence, 1 arousal, 2 dominance, 3 liking\n",
    "    if label==\"arousal\":\n",
    "        label=1\n",
    "    elif label==\"valence\":\n",
    "        label=0\n",
    "    label_in= data_file_in[\"labels\"][:,label]>5\n",
    "    label_inter\t= np.empty([0]) # initial empty data arrays\n",
    "    data_inter_cnn\t= np.empty([0,window_size, 9, 9])\n",
    "    data_inter_rnn\t= np.empty([0, window_size, 32])\n",
    "    trials = data_in.shape[0]\n",
    "\n",
    "    # Data pre-processing\n",
    "    for trial in range(0,trials):\n",
    "        if yes_or_not==\"yes\":\n",
    "            base_signal = (data_in[trial,0:128,0:32]+data_in[trial,128:256,0:32]+data_in[trial,256:384,0:32])/3\n",
    "        else:\n",
    "            base_signal = 0\n",
    "        data = data_in[trial,384:8064,0:32]\n",
    "        # compute the deviation between baseline signals and experimental signals\n",
    "        for i in range(0,60):\n",
    "            data[i*128:(i+1)*128,0:32]=data[i*128:(i+1)*128,0:32]-base_signal\n",
    "        label_index = trial\n",
    "        #read data and label\n",
    "        data = norm_dataset(data)\n",
    "        data, label = segment_signal_without_transition(data, label_in,label_index,window_size)\n",
    "        # cnn data process\n",
    "        data_cnn    = dataset_1Dto2D(data)\n",
    "        data_cnn    = data_cnn.reshape ( int(data_cnn.shape[0]/window_size), window_size, 9, 9)\n",
    "        # rnn data process\n",
    "        data_rnn    = data. reshape(int(data.shape[0]/window_size), window_size, 32)\n",
    "        # append new data and label\n",
    "        data_inter_cnn  = np.vstack([data_inter_cnn, data_cnn])\n",
    "        data_inter_rnn  = np.vstack([data_inter_rnn, data_rnn])\n",
    "        label_inter = np.append(label_inter, label)\n",
    "    '''\n",
    "    print(\"total cnn size:\", data_inter_cnn.shape)\n",
    "    print(\"total rnn size:\", data_inter_rnn.shape)\n",
    "    print(\"total label size:\", label_inter.shape)\n",
    "    '''\n",
    "    # shuffle data\n",
    "    index = np.array(range(0, len(label_inter)))\n",
    "    np.random.shuffle( index)\n",
    "    shuffled_data_cnn\t= data_inter_cnn[index]\n",
    "    shuffled_data_rnn\t= data_inter_rnn[index]\n",
    "    shuffled_label \t= label_inter[index]\n",
    "    return shuffled_data_cnn ,shuffled_data_rnn,shuffled_label,record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time begin: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=41, tm_sec=38, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s01.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=42, tm_sec=9, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 31.070467233657837\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s15.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=42, tm_sec=39, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 60.94023513793945\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s29.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=43, tm_sec=12, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 93.29165124893188\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s28.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=43, tm_sec=41, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 122.5144362449646\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s14.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=44, tm_sec=19, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 160.2364580631256\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s16.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=44, tm_sec=49, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 190.36706709861755\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s02.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=45, tm_sec=17, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 218.45500230789185\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s03.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=45, tm_sec=46, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 247.8390190601349\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s17.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=46, tm_sec=17, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 278.7851243019104\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s13.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=46, tm_sec=47, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 308.865385055542\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s07.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=47, tm_sec=15, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 336.7351360321045\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s06.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=47, tm_sec=43, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 364.6484181880951\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s12.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=48, tm_sec=11, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 392.67243218421936\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s04.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=48, tm_sec=39, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 420.4668860435486\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s10.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=49, tm_sec=7, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 448.64809107780457\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s11.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=49, tm_sec=35, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 476.7788691520691\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s05.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=50, tm_sec=3, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 504.63174200057983\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s20.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=50, tm_sec=31, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 532.6049559116364\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s08.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=50, tm_sec=59, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 560.8541121482849\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s09.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=51, tm_sec=27, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 588.917426109314\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s21.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=51, tm_sec=56, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 617.6005802154541\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s23.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=52, tm_sec=24, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 645.8552992343903\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s22.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=52, tm_sec=52, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 673.9844219684601\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s32.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=53, tm_sec=20, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 702.0878002643585\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s26.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=53, tm_sec=48, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 730.03089427948\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s27.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=54, tm_sec=18, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 759.2186172008514\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s19.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=54, tm_sec=46, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 787.290069103241\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s25.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=55, tm_sec=14, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 815.8702480792999\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s31.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=55, tm_sec=45, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 846.5918810367584\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s30.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=56, tm_sec=13, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 874.9209752082825\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s24.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=56, tm_sec=42, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 903.6732492446899\n",
      "Processing /Users/zouhao/Desktop/EEGResearch/Dataset/s18.mat ..........\n",
      "end time: time.struct_time(tm_year=2020, tm_mon=7, tm_mday=21, tm_hour=16, tm_min=57, tm_sec=11, tm_wday=1, tm_yday=203, tm_isdst=1)\n",
      "time consuming: 932.7631340026855\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "print(\"time begin:\",time.localtime())\n",
    "dataset_dir = \"/Users/zouhao/Desktop/EEGResearch/Dataset/\"\n",
    "window_size = 128\n",
    "output_dir = \"/Users/zouhao/Desktop/EEGResearch/deap_shuffled_data/\"\n",
    "\n",
    "label_class = 'arousal'     # arousal/valence\n",
    "suffix = 'yes'     # yes/no (using baseline signals or not)\n",
    "# get directory name for one subject\n",
    "record_list = [task for task in os.listdir(dataset_dir) if os.path.isfile(os.path.join(dataset_dir,task))]\n",
    "output_dir = output_dir+suffix+\"_\"+label_class+\"/\"\n",
    "if os.path.isdir(output_dir)==False:\n",
    "    os.makedirs(output_dir)\n",
    "# print(record_list)\n",
    "\n",
    "for record in record_list:\n",
    "    file = os.path.join(dataset_dir,record)\n",
    "    shuffled_cnn_data,shuffled_rnn_data,shuffled_label,record = apply_mixup(file, window_size,label_class,suffix)\n",
    "    output_data_cnn = output_dir+record+\"_win_\"+str(window_size)+\"_cnn_dataset.pkl\"\n",
    "    output_data_rnn = output_dir+record+\"_win_\"+str(window_size)+\"_rnn_dataset.pkl\"\n",
    "    output_label= output_dir+record+\"_win_\"+str(window_size)+\"_labels.pkl\"\n",
    "\n",
    "    with open(output_data_cnn, \"wb\") as fp:\n",
    "        pickle.dump( shuffled_cnn_data,fp, protocol=4)\n",
    "    with open( output_data_rnn, \"wb\") as fp:\n",
    "        pickle.dump(shuffled_rnn_data, fp, protocol=4)\n",
    "    with open(output_label, \"wb\") as fp:\n",
    "        pickle.dump(shuffled_label, fp)\n",
    "    end = time.time()\n",
    "    print(\"end time:\",time.localtime())\n",
    "    print(\"time consuming:\",(end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. PCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CNN works for mining cross-channel correlation and extracting features from 2D frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 3 continuous 2D convolutional layers with a same kernel size of 4 * 4 for spatial feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fuse = \"concat\"\n",
    "\n",
    "conv_1_shape = '4*4*32'\n",
    "pool_1_shape = 'None'\n",
    "\n",
    "conv_2_shape = '4*4*64'\n",
    "pool_2_shape = 'None'\n",
    "\n",
    "conv_3_shape = '4*4*128'\n",
    "pool_3_shape = 'None'\n",
    "\n",
    "conv_4_shape = '1*1*13'\n",
    "pool_4_shape = 'None'\n",
    "\n",
    "window_size = 128\n",
    "n_lstm_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN (LSTM) models the context information for streaming 1D data vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. feature fusion method: fuse the extracted features at last for final emotion recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_fuse = \"concat\"\n",
    "\n",
    "conv_1_shape = '4*4*32'\n",
    "pool_1_shape = 'None'\n",
    "\n",
    "conv_2_shape = '4*4*64'\n",
    "pool_2_shape = 'None'\n",
    "\n",
    "conv_3_shape = '4*4*128'\n",
    "pool_3_shape = 'None'\n",
    "\n",
    "conv_4_shape = '1*1*13'\n",
    "pool_4_shape = 'None'\n",
    "\n",
    "window_size = 128\n",
    "n_lstm_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size of hidden state 32\n"
     ]
    }
   ],
   "source": [
    "# lstm full connected parameter\n",
    "n_hidden_state = 32\n",
    "print(\"\\nsize of hidden state\", n_hidden_state)\n",
    "n_fc_out = 1024\n",
    "n_fc_in = 1024\n",
    "\n",
    "dropout_prob = 0.5\n",
    "np.random.seed(32)\n",
    "\n",
    "norm_type = '2D'\n",
    "regularization_method = 'dropout'\n",
    "enable_penalty = True\n",
    "\n",
    "cnn_suffix        =\".mat_win_128_cnn_dataset.pkl\"\n",
    "rnn_suffix        =\".mat_win_128_rnn_dataset.pkl\"\n",
    "label_suffix    =\".mat_win_128_labels.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arousal_or_valence = 'arousal'\n",
    "with_or_without = 'no'\n",
    "dataset_dir = \"/Users/zouhao/Desktop/EEGResearch/Parallel_CRNN/deep_suffled_data/\"+with_or_without+\"_\"+arousal_or_valence+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/zouhao/Desktop/EEGResearch/Parallel_CRNN/deep_suffled_data/no_arousal/.mat_win_128_cnn_dataset.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a949dcf28ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m###load training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mcnn_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcnn_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mrnn_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrnn_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/zouhao/Desktop/EEGResearch/Parallel_CRNN/deep_suffled_data/no_arousal/.mat_win_128_cnn_dataset.pkl'"
     ]
    }
   ],
   "source": [
    "###load training set\n",
    "with open(dataset_dir  + cnn_suffix, \"rb\") as fp:\n",
    "    cnn_datasets = pickle.load(fp)\n",
    "with open(dataset_dir  + rnn_suffix, \"rb\") as fp:\n",
    "    rnn_datasets = pickle.load(fp)\n",
    "with open(dataset_dir  + label_suffix, \"rb\") as fp:\n",
    "    labels = pickle.load(fp)\n",
    "    labels = np.transpose(labels)\n",
    "    print(\"loaded shape:\",labels.shape)\n",
    "lables_backup = labels\n",
    "print(\"cnn_dataset shape before reshape:\", np.shape(cnn_datasets))\n",
    "cnn_datasets = cnn_datasets.reshape(len(cnn_datasets), window_size, 9,9, 1)\n",
    "print(\"cnn_dataset shape after reshape:\", np.shape(cnn_datasets))\n",
    "one_hot_labels = np.array(list(pd.get_dummies(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import xlwt\n",
    "import argparse\n",
    "\n",
    "out_book = xlwt.Workbook(encoding='utf-8', style_compression=0)\n",
    "out_sheet = out_book.add_sheet('accuracy', cell_overwrite_ok=True)\n",
    "column_index = 0\n",
    "persons = 32\n",
    "fold =10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_heaer(column_index,model_name,target_class):\n",
    "    # table header\n",
    "    out_sheet.write(0,column_index,\"model_name:\")\n",
    "    out_sheet.write(0,column_index+1,model_name)\n",
    "    out_sheet.write(1,column_index,\"target_class:\")\n",
    "    out_sheet.write(1,column_index+1,target_class)\n",
    "    out_sheet.write(2,column_index,\"subject\")\n",
    "    out_sheet.write(2,column_index+1,\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cells(dir_path,column_index,model_name,target_class):\n",
    "    fill_heaer(column_index,model_name,target_class)\n",
    "    total_accuracy = 0\n",
    "    for sub in range(1,persons+1):\n",
    "        subject = \"s%02d\"%sub\n",
    "        accuracy = 0\n",
    "        for count in range(fold):\n",
    "            input_file = dir_path+target_class+\"/\"+str(subject)+\"_\"+str(count)+\".xlsx\"\n",
    "            in_book = xlrd.open_workbook(input_file)\n",
    "            sheet = in_book.sheet_by_name(\"condition\")\n",
    "            accuracy += sheet.cell_value(1,0)\n",
    "        accuracy = (accuracy/10)*100\n",
    "        total_accuracy += accuracy\n",
    "        print(sub,\":\",accuracy)\n",
    "        out_sheet.write(sub + 2,column_index, subject)\n",
    "        out_sheet.write(sub + 2,column_index+1,accuracy)\n",
    "    mean_accuracy = total_accuracy/persons\n",
    "    print(\"mean accuracy:\",mean_accuracy)\n",
    "    out_sheet.write(sub+3,column_index,\"mean:\")\n",
    "    out_sheet.write(sub+3,column_index+1,mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cells_origin(dir_path,column_index,model_name,target_class):\n",
    "    # table header\n",
    "    fill_heaer(column_index,model_name,target_class)\n",
    "    total_accuracy = 0\n",
    "    for sub in range(1,persons+1):\n",
    "        subject = \"s%02d\"%sub\n",
    "        accuracy = 0\n",
    "        input_file = dir_path+target_class+\"/\"+str(subject)+\".xlsx\"\n",
    "        in_book = xlrd.open_workbook(input_file)\n",
    "        sheet = in_book.sheet_by_name(\"condition\")\n",
    "        accuracy += sheet.cell_value(1,0)\n",
    "        accuracy = (accuracy)*100\n",
    "        total_accuracy += accuracy\n",
    "        print(sub,\":\",accuracy)\n",
    "        out_sheet.write(sub + 2,column_index, subject)\n",
    "        out_sheet.write(sub + 2,column_index+1,accuracy)\n",
    "    mean_accuracy = total_accuracy/persons\n",
    "    print(\"mean accuracy:\",mean_accuracy)\n",
    "    out_sheet.write(sub+3,column_index,\"mean:\")\n",
    "    out_sheet.write(sub+3,column_index+1,mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
